# 线性回归

**给定训练集**：要经行监督学习必须确定**假设$h$**的表示，把$y$假设为一个以$x$为自变量的线性函数：
$$
h_{\theta}(x)=\theta_{0}+\theta_{1} \times x_{1}+\theta_{2} \times x_{2}
$$

函数中$\theta_{i}$是**参数(权重)**，是从$x$映射到$y$的线性函数映射的空间参数，不引起混淆时将$h_{\theta}(x)$写成$h(x)$，简化公式设$x_{0}$=1(**截距项**)得到：
$$
h(x)=\sum_{i=0}^{n} \theta_{i} x_{i}=\theta^{T} x
$$
等式最右边$\theta$和$x$为向量，等式中$n$是输入变量个数**（不包括$x_0$）** 

**确定参数$\theta$**：让$h(x)$尽量逼近$y$

公式表示：就要定义一个函数，来衡量对于每个不同的 $\theta$ 值，$h(x^{(i)})$ 与对应的 $y^{(i)}$ 的距离。这样用如下的方式定义了一个 **成本函数 （cost function**）:

$$
J(\theta) = \frac 12 \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2
$$
**最小化参数$\theta$**有以下方法：

### 1.最小均方算法

**梯度下降法（gradient descent algorithm）**：对$\theta$给定初值，对不$\theta$断调整让$J(\theta)$变小直到找到使$J(\theta)$最小的$\theta$，从某一个 $\theta$ 的初始值开始，然后逐渐重复更新：
$$
\theta_j := \theta_j - \alpha \frac \partial {\partial\theta_j}J(\theta)
$$
$\alpha$ 称为学习速率
$$
\begin{align}
\frac \partial {\partial\theta_j}J(\theta) & = \frac \partial {\partial\theta_j} \frac  12(h_\theta(x)-y)^2\\
& = 2 \cdot\frac 12(h_\theta(x)-y)\cdot \frac \partial {\partial\theta_j}  h_\theta(x)    \\
& = (h_\theta(x)-y)\cdot \frac \partial {\partial\theta_j}(\sum^n_{i=0} \theta_ix_i)&\text{这里数据集}h(x)=\sum_{i=0}^{n} \theta_{i} x_{i}\\
& = (h_\theta(x)-y) x_j
\end{align}
$$


**LMS 更新规则 （least mean squares最小均方）**

对于单个样本将上式$\frac{\partial}{\partial \theta_{j}} J(\theta)$带入公式得到更新规则：
$$
\theta_j := \theta_j + \alpha (y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}
$$
对于多个样本只需要重复该过程直到收敛

**批量梯度下降算法（batch gradient descent）**
$$
\begin{aligned}
&\qquad 重复直到收敛 \{ \\
&\qquad\qquad\theta_j := \theta_j + \alpha \sum^m_{i=1}(y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\quad(对每个j) \\
&\qquad\}
\end{aligned}
$$

用于收敛到全局最小值容易被局部最小值影响    $J$ 是一个凸二次函数。

**随机梯度下降法（stochastic gradient descent）**
$$
\begin{aligned}
&\qquad循环：\{ \\
&\qquad\qquad i从1到m,\{   \\
&\qquad\qquad\qquad\theta_j := \theta_j  +\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)} \quad(对每个 j) \\
&\qquad\qquad\}  \\
&\qquad\}
\end{aligned}
$$

批量梯度下降法要在运行第一步之前先对整个训练集进行扫描遍历，当训练集的规模 *m* 变得很大的时候，引起的性能开销就很不划算了；随机梯度下降法就没有这个问题，而是可以立即开始，对查询到的每个样本都进行运算。通常情况速度更快，可能无法收敛$\theta$会一直在 $J(\theta)$ 最小值附近震荡**训练集很大的情况下，随机梯度下降往往比批量梯度下降更受青睐。**

### 2.法方程

不需要使用迭代算法，直接找对应导数为0位置的$\theta_j$从而找到$J$的最小值

#### 2.1矩阵导数

**梯度 $\nabla_A f(A)$**：假如有一个函数 $f: R^{m\times n} → R$ 从 $m\times n$ 大小的矩阵映射到实数域，那么就可以定义当矩阵为 $A$ 的时候有导函数 $f$ 如下所示：
$$
\nabla_A f(A)=\begin{bmatrix} \frac {\partial f}{\partial A_{11}} & \dots  & \frac {\partial f}{\partial A_{1n}} \\ \vdots  & \ddots & \vdots  \\ \frac {\partial f}{\partial A_{m1}} & \dots  & \frac {\partial f}{\partial A_{mn}} \\ \end{bmatrix}
$$
**$trace$求迹运算**：对于一个给定的 $n\times n$ 方形矩阵 $A$，它的迹定义为对角项和：
$$
trA = \sum^n_{i=1} A_{ii}
$$
求迹运算的一些性质：

当矩阵 $A$ 和$B$为方阵，$a$为实数
$$
\begin{align}
trAB &= trBA    &\text{（1）}\\
AB&=trCAB=trBCA    &\text{（2）}\\
trABCD=trDABC&=trCDAB=trBCDA   &\text{（3）}\\
trA&=trA^T    &\text{（4）}\\
tr(A+B)&=trA+trB   &\text{（5）}\\
traA&=a trA   & \text{（6）}
\end{align}
$$

一些矩阵导数:
$$
\begin{aligned}
   \nabla_A tr AB & = B^T & \text{(7)}\\
   \nabla_{A^T} f(A) & = (\nabla_{A} f(A))^T &\text{(8)}\\
   \nabla_A tr ABA^TC& = CAB+C^TAB^T &\text{(9)}\\
   \nabla_A|A| & = |A|(A^{-1})^T &\text{(10)}\\
\end{aligned}
$$
证明：
$$
A=\left[\begin{array}{ccc}{a_{11}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {a_{n 1}} & {\cdots} & {a_{n n}}\end{array}\right]\
B=\left[\begin{array}{ccc}{b_{11}} & {\cdots} & {b_{1 n}} \\ {\vdots} & {\ddots} & {\vdots} \\ {b_{n 1}} & {\cdots} & {b_{n n}}\end{array}\right]
$$

+ $(1)trAB = trBA$

$$
\begin{aligned} 
\operatorname{tr} A B 
&=a_{11} b_{11}+a_{12} b_{21}+\cdots+a_{1 n} b_{n 1} \\ 
&+a_{21} b_{12}+a_{22} b_{22}+\cdots+a_{2n} b_{n2} \\ 
&+a_{n1} b_{1n}+a_{n2} b_{2n}+\cdots+a_{n n} b_{n n}
=\sum_{j=1}^{n} \sum_{i=1}^{n} a_{i j} b_{ji}\\
\end{aligned}
$$

$$
\begin{aligned} 
\operatorname{tr} B A 
&=a_{11} b_{11}+a_{21} b_{12}+\cdots+a_{n 1} b_{1 n} \\ 
&+a_{12} b_{21}+a_{22} b_{22}+\cdots+a_{n 2} b_{2 n} \\ 
&+a_{1 n} b_{n 1}+a_{2 n} b_{n 2}+\cdots+a_{n n} b_{n n}
=\sum_{j=1}^{n} \sum_{i=1}^{n} a_{i j} b_{i j}\\
\end{aligned}
$$

此时两矩阵为方阵所以二者值相等

+ $(7)\nabla_{A} \operatorname{tr} A B=B^{T}$

  $t r A B=\sum_{j=1}^{n} \sum_{j=1}^{n} a_{i j} b_{j i}$                对$a_{ij}$取梯度得$\frac{d t r A B} {d a_{i j}}=b_{j i}$             所以

  $\nabla_{A} \operatorname{tr} A B=\left[\begin{array}{ccc}{b_{11}} & {\cdots} & {b_{n 1}} \\ {\vdots} & {\ddots} & {\vdots} \\ {b_{1 n}} & {\cdots} & {b_{n n}}\end{array}\right]=B^{T}$

#### 2.2最小二乘法回顾

**给定训练集** ：忽略截距项$x$设置为一个$m$x$n$矩阵每一行为训练样本输入值，$\vec{y}$ 是一个 $m$ 维向量包含训练集所有目标值         $h_\theta (x^{(i)}) = (x^{(i)})^T\theta $
$$
X=
\left[\begin{array}{c}{-\left(x^{(1)}\right)^{T}-} \\ {-\left(x^{(2)}\right)^{T}-} \\ {\vdots} \\ {-\left(x^{(m)}\right)^{T}-}\end{array}\right]\
y=
\left[\begin{array}{c}{y^{(1)}} \\ {y^{(2)}} \\ {\vdots} \\ {y^{(m)}}\end{array}\right]\\
\begin{aligned} 
X \theta-\vec{y} &=\left[\begin{array}{c}{\left(x^{(1)}\right)^{T} \theta} \\ {\vdots} \\ 
{\left(x^{(m)}\right)^{T} \theta}\end{array}\right]-\left[\begin{array}{c}{y^{(1)}} \\ {\vdots} \\ {y^{(m)}}\end{array}\right]
=\left[\begin{array}{c}{h_{\theta}\left(x^{1}\right)-y^{(1)}} \\ {\vdots} \\ {h_{\theta}\left(x^{m}\right)-y^{(m)}}\end{array}\right] 
\end{aligned}
$$
对于向量 $\vec{z}$ ，则有 $z^T z = \sum_i z_i^2$ ，因此利用这个性质，可以推出:
$$
\begin{aligned}
\frac 12(X\theta - \vec{y})^T (X\theta - \vec{y}) &=\frac 12 \sum^m_{i=1}(h_\theta (x^{(i)})-y^{(i)})^2\\
&= J(\theta)
\end{aligned}
$$
使用梯度最小化$J(\theta)$ 

#### ！！不是很懂

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \nabla_\theta \frac 12 (X\theta - \vec{y})^T (X\theta - \vec{y}) \\
&= \frac  12 \nabla_\theta (\theta ^TX^TX\theta -\theta^T X^T \vec{y} - \vec{y} ^TX\theta +\vec{y}^T \vec{y})\\
&= \frac  12 \nabla_\theta tr(\theta ^TX^TX\theta -\theta^T X^T \vec{y} - \vec{y} ^TX\theta +\vec{y}^T \vec{y})\\
&= \frac  12 \nabla_\theta (tr \theta ^TX^TX\theta - 2tr\vec{y} ^T X\theta)\\
&= \frac  12 (X^TX\theta+X^TX\theta-2X^T\vec{y}) \\
&= X^TX\theta-X^T\vec{y}\\
\end{aligned}
$$

### 3.概率解释

证明为何选最小二乘法作为成本函数

假设目标变量和输入值存在以下关系：
$$
y^{(i)}=\theta^T x^{(i)}+ \epsilon ^{(i)}\\
$$
 $ \epsilon ^{(i)}$ 是误差项，用于存放由于建模所忽略的变量导致的效果 （比如可能某些特征对于房价的影响很明显，但我们做回归的时候忽略掉了）或者随机的噪音信息。进一步假设 $ \epsilon ^{(i)}$   是独立同分布的服从高斯分布，其平均值为 $0$，方差为 $\sigma ^2$。这样就可以把这个假设写成 $ \epsilon ^{(i)} ∼ N (0, \sigma ^2)$ 。然后 $ \epsilon ^{(i)} $  的密度函数就是：
$$
p(\epsilon ^{(i)} )= \frac 1{\sqrt{2\pi}\sigma} exp (- \frac  {(\epsilon ^{(i)} )^2}{2\sigma^2})
$$
将目标值和输入值的关系带入等式得到：
$$
p(y ^{(i)} |x^{(i)}; \theta)= \frac 1{\sqrt{2\pi}\sigma} exp (- \frac  {(y^{(i)} -\theta^T x ^{(i)} )^2}{2\sigma^2})
$$
 $p(y ^{(i)} |x^{(i)}; \theta)$ 表示的是这是一个对于给定 $x^{(i)}$ 时 $y^{(i)}$ 的分布               $\theta$ 代表该分布的参数

 $y^{(i)}$  的分布可以写成$y^{(i)} | x^{(i)}; \theta ∼ N (\theta ^T x^{(i)}, \sigma^2)$

$p (\vec{y}|X;\theta )$表示为给定矩阵$X$，其包含了所有的$x^{(i)}$，然后再给定 $\theta$ 时 $y^{(i)}$ 的分布，当在$\theta$取某个固定值的情况下，这个等式通常可以看做是一个 $\vec{y}$ 的函数（也可以看成是 $X$ 的函数）。**当我们要把它当做 $\theta$ 的函数的时候，就称它为似然函数**，可以表示为
$$
L(\theta) =L(\theta;X,\vec{y})=p(\vec{y}|X;\theta)
$$
因为$\epsilon^{(i)}$服从独立分布等式可以表示为
$$
\begin{aligned}
L(\theta) &=\prod ^m _{i=1}p(y^{(i)}|x^{(i)};\theta)\\
&=\prod ^m _{i=1} \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
\end{aligned}
$$
这时候需要选择最佳参数$\theta$ 即让似然函数取最大值通常对等式两边取对数简化运算
$$
\begin{aligned}
l(\theta) &=\log L(\theta)\\
&=\log \prod ^m _{i=1} \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= \sum ^m _{i=1}log \frac  1{\sqrt{2\pi}\sigma} exp(- \frac {(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= m \log \frac  1{\sqrt{2\pi}\sigma}- \frac 1{\sigma^2}\cdot \frac 12 \sum^m_{i=1} (y^{(i)}-\theta^Tx^{(i)})^2\\
\end{aligned}
$$
对 $\theta$ 的选择并不依赖 $\sigma^2$ 所以只看减号后面的没有$\sigma^2$ 的部分

对 $l(\theta)$ 取得最大值也就意味着下面这个子式取到最小值：
$$
\frac 12 \sum^m _{i=1} (y^{(i)}-\theta^Tx^{(i)})^2
$$
到这里我们能发现这个子式实际上就是 $J(\theta)$，也就是最原始的最小二乘成本函数

### 4.局部加权线性回归

一个学习算法要保证能良好运行，特征的选择是非常重要的。局部加权线性回归（locally weighted linear regression ，缩写为LWR），这个方法是假设有足够多的训练数据，对不太重要的特征进行一些筛选。

在原始版本的线性回归算法中，要对一个查询点 $x$ 进行预测，比如要衡量$h(x)$，要经过下面的步骤：

1. 使用参数 $\theta$ 进行拟合，让数据集中的值与拟合算出的值的差值平方$\sum_i(y^{(i)} − \theta^T x^{(i)} )^2$最小(最小二乘法的思想)；
2. 输出 $\theta^T x$ 。

相应地，在 LWR 局部加权线性回归方法中，步骤如下：

1. 使用参数 $\theta$ 进行拟合，让加权距离$\sum_i w^{(i)}(y^{(i)} − \theta^T x^{(i)} )^2$ 最小；
2. 输出 $\theta^T x$。

上面式子中的 $w^{(i)}$ 是非负的权值。如果对应某个$i$ 的权值 $w^{(i)}$ 特别大，那么在选择拟合参数 $\theta$ 的时候，就要尽量让这一点的 $(y^{(i)} − \theta^T x^{(i)} )^2$ 最小。而如果权值$w^{(i)}$  特别小，那么这一点对应的$(y^{(i)} − \theta^T x^{(i)} )^2$ 就基本在拟合过程中忽略掉了。

对于权值的选取可以使用下面这个比较标准的公式：
$$
w^{(i)} = exp(- \frac {(x^{(i)}-x)^2}{2\tau^2})
$$

> 如果 $x$ 是有值的向量，那就要对上面的式子进行泛化，得到的是$w^{(i)} = exp(− \frac {(x^{(i)}-x)^T(x^{(i)}-x)}{2\tau^2})$，或者:$w^{(i)} = exp(− \frac {(x^{(i)}-x)^T\Sigma ^{-1}(x^{(i)}-x)}{2})$，这就看是选择用$\tau$ 还是 $\Sigma$。

要注意的是，权值是依赖每个特定的点 $x$ 的，而这些点正是我们要去进行预测评估的点。此外，如果 $|x^{(i)} − x|$ 非常小，那么权值 $w^{(i)} $就接近 $1$；反之如果 $|x^{(i)} − x|$ 非常大，那么权值 $w^{(i)} $就变小。所以可以看出， $\theta$ 的选择过程中，查询点 $x$ 附近的训练样本有更高得多的权值。；

$\tau$也叫做**带宽参数**，随着点$x^{(i)} $ 到查询点 $x$ 的距离降低，训练样本的权值的也在降低，参数$\tau$  控制了这个降低的速度。

局部加权线性回归是咱们接触的第一个**非参数** 算法。而更早之前咱们看到的无权重的线性回归算法就是一种**参数** 学习算法，因为有固定的有限个数的参数（也就是 $\theta_i$ ），这些参数用来拟合数据。我们对 $\theta_i$ 进行了拟合之后，就把它们存了起来，也就不需要再保留训练数据样本来进行更进一步的预测了。

与之相反，如果用局部加权线性回归算法，我们就必须一直保留着整个训练集。这里的非参数算法中的 非参数“non-parametric” 是粗略地指：为了呈现出假设 $h$ 随着数据集规模的增长而线性增长，我们需要以一定顺序保存一些数据的规模。





