# 生成学习算法

目前为止，我们讲过的学习算法的模型都是$p (y|x;\theta)$（概率派），也就是给定 $x$ 下 $y$ 的条件分布，以  $\theta$  为参数。例如，逻辑回归中就是以 $h_\theta(x) = g(\theta^T x)$ 作为 $p (y|x;\theta)$ 的模型，这里的 $g$ 是一个 $S$型函数。接下来，咱们要讲一下一种不同类型的学习算法。

+ **判别式学习算法**：逻辑回归之类的直接试图建立 $p(y|x)$的算法，以及感知器算法等直接用投图的思路来判断对应 $X$ 的值落到了 $\{0, 1\}$ 中哪个区域的算法。
+ **生成学习算法**：对 $p(x|y)$ 和 $p(y)$来进行建模，例如如果 $y$ 用来表示一个样例是  小狗 $(0)$ 或者  大象 $(1)$，那么$p(x|y = 0)$就是对小狗特征分布的建模，而$p(x|y = 1)$就是对大象特征分布的建模。

对 $p(y)$ **(先验)** 和$p(x|y)$ 进行建模之后，我们的算法就是用**贝叶斯规则**来推导对应给定 $x$ 下 $y$ 的**后验分布**：
$$
\begin{aligned}
\arg \max_y p(y|x) & =\arg \max_y \frac{p(x|y)p(y)}{p(x)}\\
&= \arg \max_y p(x|y)p(y)
\end{aligned}P(y|x) :后验概率，一般是我们求解的目标。
$$

+ P(y|x) :**后验概率**，一般是我们求解的目标。

+ P(x|y) :**条件概率**，又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。

+ P(y) ：**先验概率**，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。

+ P(x) ：**先验概率**，只是在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。  

在拥有训练集的情况下，显然除了后验概率P(y|x)中的x来自一篇新文章无法得到，p(x), p(y), p(x|y)都是可以在抽样集合上统计出的
$$
p(y|x)=\frac{p(x|y)p(y)}{p(x)}
$$

> 后验:知果求因）
>
> **先验概率**：事件发生前的预判概率。可以是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。一般都是单独事件概率，如P(x),P(y)。  
>
> **后验概率**：事件发生后求的反向条件概率；或者说，基于先验概率求得的反向条件概率。概率形式与条件概率相同。  
>
> **条件概率**：一个事件发生后另一个事件发生的概率。一般的形式为P(x|y)表示y发生的条件下x发生的概率。

这里的**分母（denominator）** 为：$p(x) = p(x|y = 1)p(y = 1) + p(x|y = 0)p(y = 0)$（这个等式关系可以根据概率的标准性质来推导验证`译者注：其实就是条件概率`），这样接下来就可以把它表示成我们熟悉的 $p(x|y)$和 $p(y)$ 的形式了。实际上如果我们计算$p(y|x)$ 来进行预测，那就并不需要去计算这个分母，因为有下面的等式关系：
$$
\begin{aligned}
\arg \max_y p(y|x) & =\arg \max_y \frac{p(x|y)p(y)}{p(x)}\\
&= \arg \max_y p(x|y)p(y)
\end{aligned}
$$

### 1. 高斯判别分析（GDA）

#### 1.1 多元正态分布

$n$维多元正态分布，也叫做多变量高斯分布，参数为一个$n$维 **均值向量** $\mu \in  R^n $，以及一个 **协方差矩阵** $\Sigma \in  R^{n\times n}$，其中$\Sigma \geq 0$ 是一个对称的半正定矩阵。当然也可以写成"$N (\mu, \Sigma)$" 的分布形式，密度函数为：
$$
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
$$
在上面的等式中，"$|\Sigma|$"的意思是矩阵$\Sigma$的行列式（determinant）。对于一个在 $N(\mu,\Sigma)$分布中的随机变量 $X$ ，其平均值（跟正态分布里面差不多，所以并不意外）就是 $\mu$ 了：
$$
E[X]=\int_x xp(x;\mu,\Sigma)dx=\mu
$$
随机变量$Z$是一个有值的向量（vector-valued random variable），$Z$ 的 **协方差（covariance）** 的定义是：$Cov(Z) = E[(Z-E[Z])(Z-E[Z])^T ]$。这是对实数随机变量的方差（variance）这一概念的泛化扩展。这个协方差还可以定义成$Cov(Z) = E[ZZ^T]-(E[Z])(E[Z])^T$（你可以自己证明一下这两个定义实际上是等价的。）如果 $X$ 是一个多变量正态分布，即 $X \sim N (\mu, \Sigma)$，则有：
$$
Cov(X)=\Sigma
$$
**在高斯分布的密度图中μ决定中心位置， Σ决定投影椭圆的朝向和大小** ![批注 2019-07-18 111756](https://raorui.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CS229/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/%E6%89%B9%E6%B3%A8%202019-07-18%20111756.png?Expires=1563423515&OSSAccessKeyId=TMP.hV4uyMqTUFdF7W2zPdeYSRrdaaT624dLyKBzcaQPeNpVfkkvheQ8AfkoWvDXBQWZF2HoRJMb24kHRS1jteVC4oo8Tj1g2Ex139cUqFfmNLk62rz2knvpZjvMKBFbmH.tmp&Signature=A4Sm2y1p1tlSXVTqzOBnJnySoJY%3D)

#### 1.2 高斯判别分析模型

一个分类问题，其中输入特征 $x$ 是一系列的连续随机变量，那就可以使用高斯判别分析模型（其中对 $p(x|y)$用多元正态分布）来进行建模：
$$
\begin{aligned}
y & \sim Bernoulli(\phi)\\
x|y = 0 & \sim N(\mu_o,\Sigma)\\
x|y = 1 & \sim N(\mu_1,\Sigma)\\
\end{aligned}
$$
将多元正态分布带入公式得到概率密度函数：
$$
\begin{aligned}
p(y) & =\phi^y (1-\phi)^{1-y}\\
p(x|y=0) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)  )\\
p(x|y=1) & = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} exp ( - \frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)  )\\
\end{aligned}
$$

最大似然估计如下：

$$
\begin{aligned}
l(\phi,\mu_0,\mu_1,\Sigma) &= \log \prod^m_{i=1}p(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma)\\
&= \log \prod^m_{i=1}p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)\\
\end{aligned}
$$

求到后得到以下结果：
$$
\begin{aligned}
\phi & = \frac {1}{m} \sum^m_{i=1}1\{y^{(i)}=1\}\\
\mu_0 & = \frac{\sum^m_{i=1}1\{y^{(i)}=0\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=0\}}\\
\mu_1 & = \frac{\sum^m_{i=1}1\{y^{(i)}=1\}x^{(i)}}{\sum^m_{i=1}1\{y^{(i)}=1\}}\\
\Sigma & = \frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu_{y^{(i)}})(x^{(i)}-\mu_{y^{(i)}})^T\\
\end{aligned}
$$

+ $\phi$ 是训练样本中结果$ y=1 $占有的比例 
+ $μ0$ 是$ y=0$ 的样本中特征均值 
+ $μ1$是$ y=1$ 的样本中特征均值 
+ $Σ$是样本特征方差均值 

> 注意这里的参数有两个μ，表示在不同的结果模型下，特征均值不同，但我们假设
> 协方差相同。 反映在图上就是不同模型中心位置不同，但形状相同。 这样就可以用
> 直线来进行分隔判别。 

![1563419136431](https://raorui.oss-cn-beijing.aliyuncs.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/CS229/%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/1.png?Expires=1563423393&OSSAccessKeyId=TMP.hV4uyMqTUFdF7W2zPdeYSRrdaaT624dLyKBzcaQPeNpVfkkvheQ8AfkoWvDXBQWZF2HoRJMb24kHRS1jteVC4oo8Tj1g2Ex139cUqFfmNLk62rz2knvpZjvMKBFbmH.tmp&Signature=EdOAcGP2B9DFNe0GOVdLYLPBLzA%3D)

图中展示的点就是训练数据集，图中的两个高斯分布就是针对两类数据各自进行的拟合两个高斯分布的轮廓图有同样的形状和拉伸方向，这是因为他们都有同样的协方差矩阵$\Sigma$，但他们有不同的均值$\mu_0$ 和 $\mu_1$图中的直线给出了$p (y = 1|x) = 0.5$ 这条边界线。在这条边界的一侧，我们预测 $y = 1$是最可能的结果，而另一侧，就预测 $y = 0$是最可能的结果。

#### 1.3 高斯判别分析与逻辑回归



### 2. 朴素贝叶斯法（TODO）

高斯判别分析（GDA）方法中，特征向量 $x$ 是连续的，值为实数的向量。下面我们要讲的是当 $x_i$ 是离散值的时候来使用的另外一种学习算法。

#### 2.1 拉普拉斯平滑

 #### 2.2 针对文本分类的事件模型

